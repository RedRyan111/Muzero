{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "#from networkx.drawing.nx_agraph import write_dot, graphviz_layout\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils import data as t_data\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn.functional as Func\n",
    "from torch.nn.functional import conv2d\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class representation_h(nn.Module):\n",
    "    def __init__(self,inp,out):\n",
    "        super(representation_h, self).__init__()\n",
    "        \n",
    "        #residual block 1\n",
    "        self.conv1 = nn.Conv2d(1, 5, kernel_size=1, stride=1)\n",
    "        self.conv11 = nn.Conv2d(6, 6, kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(inp, inp)\n",
    "        \n",
    "        self.fc_main1 = nn.Linear(inp,out)\n",
    "        \n",
    "    def forward(self, board):\n",
    "        \n",
    "        act = nn.Sigmoid()\n",
    "        act1 = nn.ReLU()\n",
    "        \n",
    "        x1 = torch.FloatTensor(board)\n",
    "        x1_temp = act(self.conv1(x1))\n",
    "        x1 = torch.cat((x1_temp,x1),1)\n",
    "        x1 = act(self.conv11(x1))\n",
    "        x1 = x1.flatten()\n",
    "        x1 = self.fc1(x1)\n",
    "        \n",
    "        main = act1(self.fc_main1(x1))\n",
    "        \n",
    "        return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prediction_f(nn.Module):\n",
    "    def __init__(self,f_inp,f_out1,f_out2):\n",
    "        super(prediction_f, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(f_inp, 50)\n",
    "        \n",
    "        self.fc2 = nn.Linear(50, f_out1)\n",
    "        \n",
    "        self.fc3 = nn.Linear(50, f_out2)\n",
    "        \n",
    "    def forward(self, state_rep):\n",
    "        \n",
    "        act = nn.Sigmoid()\n",
    "        act1 = nn.ReLU()\n",
    "        act2 = nn.Softmax(dim=0)\n",
    "        \n",
    "        x = act(self.fc1(state_rep))\n",
    "        \n",
    "        policy = act(self.fc2(x))\n",
    "        \n",
    "        v_value = act1(self.fc3(x))\n",
    "    \n",
    "        return policy,v_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dynamic_g(nn.Module):\n",
    "    def __init__(self,inp1,inp2,out1,out2):\n",
    "        super(dynamic_g, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(inp1+inp2, 50)\n",
    "        \n",
    "        self.fc2 = nn.Linear(50, out1)\n",
    "        \n",
    "        self.fc3 = nn.Linear(50, out2)\n",
    "\n",
    "    def forward(self, state_rep, action):\n",
    "        \n",
    "        act = nn.Sigmoid()\n",
    "        act1 = nn.ReLU()\n",
    "        \n",
    "        state_rep = state_rep.flatten()\n",
    "        action = action.flatten()\n",
    "        x = torch.cat((state_rep,action),0)\n",
    "        \n",
    "        x = act(self.fc1(x))\n",
    "        \n",
    "        pred_state_rep = act(self.fc2(x))\n",
    "        \n",
    "        pred_reward = act1(self.fc3(x))\n",
    "    \n",
    "        return pred_state_rep,pred_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class player:\n",
    "    def __init__(self):\n",
    "        self.Graph = nx.DiGraph()\n",
    "        \n",
    "        self.max_node_num = 0\n",
    "    \n",
    "    def build_tree(self,H,F,G,H_board,depth,f_out1):\n",
    "        \n",
    "        #Get available actions\n",
    "        #add edges and nodes\n",
    "        \n",
    "        root_state = H.forward(H_board)\n",
    "        policy,v_value = F.forward(root_state)\n",
    "        \n",
    "        node = 0\n",
    "        self.Graph.add_node(node,state=root_state,r = 0,policy = policy, v = v_value,N=1)\n",
    "        \n",
    "        turn = 1\n",
    "        \n",
    "        self.recurse_tree(H,F,G,root_state,node,depth,turn,f_out1)\n",
    "        \n",
    "    def recurse_tree(self,H,F,G,root_state,node,depth,turn,f_out1):\n",
    "        depth += -1  \n",
    "        \n",
    "        if(turn == 1):\n",
    "            turn = -1\n",
    "        elif(turn == -1):\n",
    "            turn = 1\n",
    "        \n",
    "        if(depth==0):\n",
    "            return    \n",
    "        \n",
    "        for i in range(2):\n",
    "            temp_node = node\n",
    "            #Get policy and  v_value\n",
    "            policy,v_value = F.forward(root_state)\n",
    "            \n",
    "            #interpret action from F\n",
    "            index = random.randint(0,len(policy)-1)\n",
    "            a_t = index_2_action(index,f_out1)\n",
    "            a_t = torch.FloatTensor(a_t)\n",
    "\n",
    "            #get new_state representation and predicted reward\n",
    "            new_state_rep, pred_reward = G.forward(root_state,a_t)      \n",
    "            \n",
    "            #check if action from state exists\n",
    "            prev_bool = False\n",
    "            temp_edge = []\n",
    "            for edge in self.Graph.out_edges(temp_node):\n",
    "                a = np.array(self.Graph.edges[edge]['action'])\n",
    "                b = np.array(index)\n",
    "                \n",
    "                if(np.array_equal(a,b)):\n",
    "                    prev_bool = True\n",
    "                    temp_edge = edge\n",
    "                    break     \n",
    "                    \n",
    "            #if edge doesn't exist, add node and edge to graph\n",
    "            if(not prev_bool):\n",
    "                self.Graph.add_node(self.max_node_num+1,N=1,state=new_state_rep,r=pred_reward,policy = policy, v = v_value)\n",
    "                self.Graph.add_edge(temp_node,self.max_node_num+1,action=index)  \n",
    "                self.max_node_num+=1\n",
    "                temp_node = self.max_node_num\n",
    "            else:\n",
    "                temp_node = temp_edge[1]\n",
    "                self.Graph.nodes[temp_node]['N']+=1\n",
    "            \n",
    "            #go to node\n",
    "            self.recurse_tree(H,F,G,new_state_rep,temp_node,depth,turn,f_out1)\n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Selection(Graph,node):\n",
    "    action = []\n",
    "    \n",
    "    stat_list = []\n",
    "    \n",
    "    #get policy from THIS node\n",
    "    edge_a = Graph.out_edges(node) \n",
    "    \n",
    "    #if leaf node, return action with highest v value\n",
    "    if(len(edge_a)==0):\n",
    "        act_list = nx.get_node_attributes(Graph,'policy')[node]\n",
    "        act_list = [[act_list[i],i] for i in range(len(act_list))]\n",
    "        act_list.sort(reverse=True)\n",
    "        #print(\"final q action sorted: \\n {} \\n\".format(act_list))\n",
    "        return act_list[0][1]\n",
    "    \n",
    "    for edge in edge_a:\n",
    "        index = np.array(Graph.edges[edge]['action'])\n",
    "        #print(\"node {} to {} with index: {}\".format(node,edge[1],index))\n",
    "    \n",
    "        # get P\n",
    "        next_P = nx.get_node_attributes(Graph,'policy')[node][index]\n",
    "        \n",
    "        #get next node\n",
    "        next_node = edge[1]\n",
    "    \n",
    "        #get state_rep of next node\n",
    "        #next_state_rep = nx.get_node_attributes(Graph,'state')[next_node]\n",
    "    \n",
    "        #get Q value given state\n",
    "        next_Q = nx.get_node_attributes(Graph,'v')[next_node]\n",
    "        \n",
    "        #get visit count of next node\n",
    "        next_N = nx.get_node_attributes(Graph,'N')[next_node]\n",
    "\n",
    "        #append to stat list\n",
    "        stat_list.append([next_Q,next_P,next_N,index])\n",
    "    \n",
    "    #choose max action\n",
    "    action = max_action(stat_list)\n",
    "    \n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_action(stat_list):\n",
    "    \n",
    "    #print(stat_list[0][0])\n",
    "    #stat_list = stat_list[0]\n",
    "    \n",
    "    c1 = 1.25\n",
    "    c2 = 19652\n",
    "    \n",
    "    #get sum of N counts\n",
    "    N_b = 0\n",
    "    for stat in stat_list:\n",
    "        N_b += stat[2] \n",
    "    N_b = math.sqrt(N_b)\n",
    "    #print(\"N_b: {}\".format(N_b))\n",
    "    \n",
    "    #get index to policy from actions taken in the edges\n",
    "    action_q_list = []\n",
    "    temp = []\n",
    "    for stat in stat_list:\n",
    "        #print(\"{} + {} * {} * {}\".format(stat[0],stat[1],(N_b/(1+stat[2])),(c1 + math.log( (N_b + c2 + 1)/c2 ,10))))\n",
    "        action_q = stat[0] + stat[1]*(N_b/(1+stat[2]))*(c1 + math.log( (N_b + c2 + 1)/c2 ,10))\n",
    "        action_q_list.append([action_q,stat[3]])\n",
    "        temp.append(stat)\n",
    "        \n",
    "    index = 0\n",
    "    action = 0\n",
    "\n",
    "    #print(action_q_list)\n",
    "    \n",
    "    action_q_list.sort(reverse=True)\n",
    "\n",
    "    #print(action_q_list)\n",
    "\n",
    "    return action_q_list[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selection(ship.Graph,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_2_action(index,f_out1):\n",
    "    a_t = np.zeros(f_out1)\n",
    "    a_t[index] = 1\n",
    "    return a_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z(reward_list,gamma):\n",
    "    z_list = []\n",
    "    \n",
    "    count = 0\n",
    "    for reward in reward_list:\n",
    "        z_list.append(reward*(gamma**count))\n",
    "    \n",
    "    return z_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_q(Graph):\n",
    "    \n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(Graph,node,action_list,value_list,policy_list,reward_list,f_out1):\n",
    "    #get root node\n",
    "    \n",
    "    #get action & append to action list\n",
    "    index = Selection(Graph,node)\n",
    "    action = index_2_action(index,f_out1)\n",
    "    action = torch.FloatTensor(action)\n",
    "    action_list.append(action)\n",
    "    \n",
    "    #get v_value and append to v_list\n",
    "    value = nx.get_node_attributes(Graph,'v')[node]\n",
    "    value = torch.FloatTensor(value)\n",
    "    value_list.append(value)\n",
    "    \n",
    "    #get policy and append to policy_list\n",
    "    policy = nx.get_node_attributes(Graph,'policy')[node]\n",
    "    policy = torch.FloatTensor(policy)\n",
    "    policy_list.append(policy)\n",
    "    \n",
    "    #get node out edges\n",
    "    edge_list = Graph.out_edges(node)\n",
    "    \n",
    "    #find which node to go to next\n",
    "    for edge in edge_list:\n",
    "        if(Graph.edges[edge]['action'] == index):\n",
    "            node = edge[1]\n",
    "            #get reward and append to reward_list\n",
    "            reward = nx.get_node_attributes(Graph,'r')[node]\n",
    "            reward = torch.FloatTensor(reward)\n",
    "            reward_list.append(reward)\n",
    "    \n",
    "    if(len(edge_list)==0):\n",
    "        return 0\n",
    "    \n",
    "    return get_trajectory(Graph,node,action_list,value_list,policy_list,reward_list,f_out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arr_2_action(array):\n",
    "    temp_list = list(array.detach())\n",
    "    maximum = max(temp_list)\n",
    "    return temp_list.index(maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing NN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_name = 'MsPacman-v0'\n",
    "env = gym.make(game_name)\n",
    "observation = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(observation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f007ec3e350>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perception(state):\n",
    "    sobel_x = torch.FloatTensor([[1,1,1],[1,1,1],[1,1,1]])\n",
    "    \n",
    "    sobel_x = sobel_x.view(1,1,3,3)\n",
    "    \n",
    "    grad_x = Func.conv2d(state, sobel_x,stride=2,padding=1)\n",
    "    \n",
    "    return grad_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAM4AAAD8CAYAAAA/rZtiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAASHUlEQVR4nO3df+wkdX3H8eerKHj4I/LjuFA4e2BOE0ragyNqUiW2VsFL60GbECCxKF+KJtDYaNMe2lSiMbFWNDVpaQ++l2IjB7RopA22UmKkJQUBPU9+eHDgEe5y3qG0/qwox7t/zOwxt7d7393PzOx+Zu71SDY7+5nZnc/M7ns/n/nszHsVEZjZdH5p3hUw6yIHjlkCB45ZAgeOWQIHjlkCB45ZgtYCR9K5krZJ2i5pQ1vrMZsHtfE7jqQjgEeBtwI7gfuAiyLi4cZXZjYHbbU4rwO2R8QTEfFz4CZgfUvrMpu5F7X0uicBT1Ue7wReP25hST59wXL0vYhYPmpGW4GzJEmXA5cDrFy5kkcffXReVTEbadmyZU+Om9dW4OwCVlYen1yW7RcRG4GNAGvXrj2gxbn00ktbqla6TZs2HVSWYz1zNLzvct1vo97jcdo6xrkPWC3pFElHAhcCt7W0LrOZa6XFiYjnJF0J/DtwBLApIh5qY11m89DaMU5E3A7c3tbrm82TzxwwSzC3UbVpTHJwudQydec3Uc9p5zdRz1msM8d9l/IZmYZbHLMErZxyM621a9fG3Xffvf9xjsOVHo5O19Xh6GXLlj0QEWeNWtYtjlkCB45ZAgeOWQIHjlkCB45Zgk78jjOJaU7Qg/mM7Exbx1x1Yd/5dxyzDDlwzBI4cMwSOHDMEvRmcKCuWZyo2FeH475zi2OWwC1OqYlvuNy/JdvSlX3X5DqSWxxJKyV9RdLDkh6S9L6y/GpJuyRtKW/rGqutWSbqtDjPAR+IiK9LejnwgKQ7ynmfjohP1q+eWZ6SAycidgO7y+kfSXqEIhGhWe81MjggaRVwBnBvWXSlpK2SNkk6pol1mOWk9hWgkl4GfBX4WER8XtIK4HtAAB8FToyIg47KhjJ5rq1m8jxcD7JtvmZ2BaikFwO3Ap+LiM8DRMSeiNgXEc8D11EkYD9IRGyMiLMi4qzly0em5zXLVp1RNQGLwCMR8alK+YmVxc4HHkyvnlme6oyq/QbwTuBbkraUZR8ELpK0hqKrtgN4T60ammWozqjafwEaMcvZO633OnHmQA4JCWeRuK+vCQlzeP8mXWZSPlfNLIETEpqVnJDQrGUOHLMEDhyzBA4cswRZDkcvlUNrHpfmpuREm8U62tB2ved1aXWT+9ctjlkCB45ZAgeOWQIHjlkCB45ZgixH1VI0PWrWxmk/XUu6N9CFfTPrfesWxyxBb1qcut8wXUuIN0td2Dez3rduccwS1G5xJO0AfgTsA56LiLMkHQvcDKyiuHz6goj4n7rrMstFUy3Ob0bEmsq1CxuAOyNiNXBn+disN9rqqq0HbiinbwDOa2k9ZnPRxOBAAF+WFMDfR8RGYEWZIhfgu8CKQ73Ajh07sj9w7sIB8rx0pd5N1rOJwHljROySdAJwh6RvV2dGRJRBdYBqJs+jjz66gWoc3hYWFg54vLi4OKeaHB5qd9UiYld5vxf4AkXmzj2DxITl/d4Rz9ufyfMlL3lJ3Woc1oaDZlyZNaduCtyXln/xgaSXAm+jyNx5G3BJudglwBfrrMfGGwTI4uLi/lZmMO3gaU/drtoK4AtFNlxeBNwYEf8m6T7gFkkLwJPABTXXYxOoBoqDpl21AicingB+fUT594G31Hntqlkkm8sxcd8ky1TTalVbmcF0jts1j/eviXVU9eaUm8OdW5vZyiIh4XHHHRfnnHNOq+vo6pnJS1kqSPoyujaP92/z5s1OSGjWJAeOWQIHTg9Uh6FH3VvzHDgd56CZjyxG1VatWjVVsrgmDgxnkfzPCQnzMm09N2/ePHaeWxyzBA4cswQOHLMEDhyzBA4cswRZjKo1oQun1KTUse3tmGSkqa/7tg63OGYJetPi5PgtOKwLdRylC/V2QkKzDnDgmCVI7qpJei1Fts6BU4G/AF4J/CHwdFn+wYi4PbmGZhlKDpyI2AasAZB0BLCLIsvNu4FPR8QnG6mhWYaaGhx4C/B4RDxZJu6YylIJCZs4ibALB7hd1dV9W6feTR3jXAhUTyW9UtJWSZskHdPQOsyyUTtwJB0JvAP4p7LoWuDVFN243cA1Y553uaT7Jd3/s5/9rG41zGaqiRbn7cDXI2IPQETsiYh9EfE8cB1FZs+DOJOndVkTgXMRlW7aIPVt6XyKzJ5mvVJrcKBMe/tW4D2V4k9IWkPxLwY7hua1pu2kebNI3JerHBIS5rZv62by/Alw3FDZO2vVyKwDOpGQsKvf1NOapFUb58Ybbzzg8cUXX9z6Ortm2s+RExL23HDQjCuz5jhwOm4QIBdffPH+VmYw7eBpjwOnR6qB4qBplwOnR6rHNZMe41iaLC5kmzYhYYp5JM2b5UH2PFubru7bperthIRmDXPgmCVw4JglcOD0QHUYetS9NS+LwYEmdOHsgjbqOIugOVz37aG4xTFL4MAxS9CbrlqO3YdhKXXM4U+b+rpv63CLY5bAgWOWwIFjlmCiwCnTPO2V9GCl7FhJd0h6rLw/piyXpM9I2l6miDqzrcqbzctEV4BKOhv4MfDZiDi9LPsE8ExEfFzSBuCYiPgzSeuAPwLWAa8H/joiXn+o11/qCtAmdOG3CBtvHu9f7StAI+Iu4Jmh4vXADeX0DcB5lfLPRuEe4JVDmW+sBYuLi/tvg8fWnjrHOCsiYnc5/V1gRTl9EvBUZbmdZdkBnJCwOcNBsri4yMLCgoOnRY0MDkTR35sq64cTErZjYWEBeCF4rB11AmfPoAtW3u8ty3cBKyvLnVyWWUsWFhYOCBhrX50zB24DLgE+Xt5/sVJ+paSbKAYHflDp0iVpIqFd3XXMIiFh3XWMCp4ct2se718T66iaKHAkbQbeDBwvaSfwYYqAuUXSAvAkcEG5+O0UI2rbgZ9S/F+OWa90IiFhEw6H4ejhblqfjnFyG47uzUmehzMf18yeT7npqT61Njly4PTAcJA4aNrnrlpPOFhmK4vAmTYh4TwO7JvI6t/EAW7dC9uaWOc8trsJ0+47JyQ0a5gDxyyBA8csgQPHLEEWgwNNyOEAtuk6pNZj1uvMcd85IaFZhnrT4tT9hmniGyqHOsxjnTm8hvOqmXWAA8csgQPHLIEDxyyBA8cswZKjapI2Ab8D7K0kI/wr4HeBnwOPA++OiP+VtAp4BNhWPv2eiHjvtJVqY4SkL1d89mU7pjWLkbtpTgKdpMX5B+DcobI7gNMj4teAR4GrKvMej4g15W3qoDHrgiUDZ1QWz4j4ckQ8Vz68hyIFlNlho4ljnEuBL1UenyLpG5K+KulN455UzeT59NNPN1ANs9mpFTiSPgQ8B3yuLNoNvCoizgDeD9wo6RWjnlvN5Ll8+fI61TCbueRTbiS9i2LQ4C1lClwi4lng2XL6AUmPA68B7q9TySaSzTWRNK+Jek7z/FGvMe06mkhIuJRZ7Lsckh5WJbU4ks4F/hR4R0T8tFK+XNIR5fSpwGrgieTamWVqyYSE1SyewB6KLJ5XAUcB3y8Xuyci3ivp94GPAL8Angc+HBH/slQl1q5dG3fffff+xzkOuebyzTwPXTvlP9VwPZctW5aekDAiLhpRPDIDXkTcCtw6QR3NOs1nDpglcOCYJXDgmCXozRWg0yab6+rVlvNISJjDwXxu769bHLMEDhyzBA4cswQOHLMEnRgcmMUvzzmeb5Vaj2nM4s9zU+qR63s84BbHLEEnWpxZDIfO4tLcuvPbMMk6Z1HvrrzHA25xzBI4cMwSOHDMEjhwzBI4cMwSdGJULVfzOOFyHupuZy5m+juOpE2S9kp6sFJ2taRdkraUt3WVeVdJ2i5pm6RzkmtmlrHUTJ4An65k7LwdQNJpwIXAr5bP+dtB8g6zPknK5HkI64GbIuLZiPgOsB14XY36mWWpzuDAlZK2ll25Y8qyk4CnKsvsLMsO4kye1mWpgwPXAh8Fory/hiIV7sQiYiOwEYr0UIn1aEwOKYxyPclzFnLY/9NIanEiYk9E7IuI54HreKE7tgtYWVn05LLMrFeSWhxJJ0bE7vLh+cBgxO02inzRnwJ+mSKT59dq13IGcviGy/Ukz1no2kmek/yx1P5MnpJ2UmTyfLOkNRRdtR3AewAi4iFJtwAPUyRjvyIi9jVWW7NMNJrJs1z+Y8DH6lTKLHc+5cYsgQPHLMGS/1YwC134twLrv2n+rcAtjlkCB45ZAgeOWQIHjlmCTlzIlsOf584icV+uf56bwx/b9uLPc80Odx6ONit5ONqsZQ4cswQOHLMEDhyzBFkOR/clj5f1l1scswSpCQlvriQj3CFpS1m+StL/Veb9XZuVN5uXJX/HkXQ28GPgsxFx+oj51wA/iIiPSFoF/Ouo5ZZYx/x/TDI72NjfcSa5dPquMiAOIknABcBv1aldir179wJwwgkn7J8ePJ7mNeo839px/fXXA3DZZZfNuSbj1R0ceBOwJyIeq5SdIukbwA+BP4+I/6y5joMMPvCD6XHzJnmN1OdbO66//vr9AZNzANUdHLgI2Fx5vBt4VUScAbyfIlXUK0Y9sZrJc9qVDj7w1VYn9TVSn2/tGA6Syy67bH8A5SQ5cCS9CPg94OZBWZkz+vvl9APA48BrRj0/IjZGxFnj+pDTqAZAShDUfb61K8fgqdNV+23g2xGxc1AgaTnwTETsk3QqRULCJ2rWcUl1P+wOlvyM6rLlZJLh6M3AfwOvlbRT0kI560IO7KYBnA1sLYen/xl4b0RM+k8HZsD4oMkpgFITEhIR7xpRditwa/1qTafusYqPdfJTDZ4c9erMgeqAwTyeb/UNtzDV4MkpkLI8V20S44ajp30NB0r+cgqYgc63OMPdq2m7W3Wfb83KtYUZlsWl0z7lxjLlS6fNmuTAMUvgwDFL0NlRNWvXY4+9cN7u6tWr51iTPLnFsYMMgmYQMNUgsoIDxw4wHDQOntEcOGYJHDhmCRw4doDhrtlw180KDhw7yHDwOGgO5uFoG8nBcmhuccwSOHDMEkxy6fRKSV+R9LCkhyS9ryw/VtIdkh4r748pyyXpM5K2S9oq6cy2N8Js1iZpcZ4DPhARpwFvAK6QdBqwAbgzIlYDd5aPAd5OkaRjNXA5cG3jtTabsyUDJyJ2R8TXy+kfAY8AJwHrgRvKxW4Aziun11Oky42IuAd4paQTG6+52RxNdYxTpsI9A7gXWBERu8tZ3wVWlNMnAU9VnrazLDPrjYmHoyW9jCKDzR9HxA+LtNGFiIhpr+KUdDlFV86scyZqcSS9mCJoPhcRny+L9wy6YOX9IOvFLmBl5eknl2UHaDKTp9msTTKqJmAReCQiPlWZdRtwSTl9CfDFSvkflKNrb6D4C5DdmPVJRBzyBrwRCGArsKW8rQOOoxhNewz4D+DYcnkBf0ORN/pbwFkTrCN88y3D2/3jPrPOcmM2nrPcmDXJgWOWwIFjlsCBY5bAgWOWIJcL2b4H/KS874vj6c/29GlbYPLt+ZVxM7IYjgaQdH+fziLo0/b0aVugme1xV80sgQPHLEFOgbNx3hVoWJ+2p0/bAg1sTzbHOGZdklOLY9YZcw8cSedK2lYm99iw9DPyI2mHpG9J2iLp/rJsZDKTHEnaJGmvpAcrZZ1NxjJme66WtKt8j7ZIWleZd1W5PdsknTPRSpY65b/NG3AExeUHpwJHAt8ETptnnRK3Ywdw/FDZJ4AN5fQG4C/nXc9D1P9s4EzgwaXqT3FJyZcoLh95A3DvvOs/4fZcDfzJiGVPKz93RwGnlJ/HI5Zax7xbnNcB2yPiiYj4OXATRbKPPhiXzCQ7EXEX8MxQcWeTsYzZnnHWAzdFxLMR8R1gO8Xn8pDmHTh9SewRwJclPVDmUoDxyUy6oo/JWK4su5ebKl3npO2Zd+D0xRsj4kyKnHJXSDq7OjOKPkFnhy+7Xv/StcCrgTXAbuCaOi8278CZKLFH7iJiV3m/F/gCRVM/LplJV9RKxpKbiNgTEfsi4nngOl7ojiVtz7wD5z5gtaRTJB0JXEiR7KMzJL1U0ssH08DbgAcZn8ykK3qVjGXoOOx8ivcIiu25UNJRkk6hyED7tSVfMIMRkHXAoxSjGR+ad30S6n8qxajMN4GHBtvAmGQmOd6AzRTdl19Q9PEXxtWfhGQsmWzPP5b13VoGy4mV5T9Ubs824O2TrMNnDpglmHdXzayTHDhmCRw4ZgkcOGYJHDhmCRw4ZgkcOGYJHDhmCf4f0SAyvKpQpy4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rgb_weights = [1,1,1]\n",
    "gray_img = np.dot(observation,rgb_weights)\n",
    "print(gray_img.shape)\n",
    "temp_obs = plt.imshow(gray_img,cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_weights = [1,1,1]\n",
    "gray_img = np.dot(observation,rgb_weights)\n",
    "temp_obs = np.reshape(gray_img,(1,1,210,160))\n",
    "temp_obs1 = perception(torch.FloatTensor(temp_obs))\n",
    "temp_obs2 = perception(torch.FloatTensor(temp_obs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 53, 40])\n"
     ]
    }
   ],
   "source": [
    "print(temp_obs2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_board = temp_obs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_inp = 3120    # size of hidden state after convolutions (made to make life easier)\n",
    "h_out = 20    # hidden state, can be any number based on model\n",
    "\n",
    "f_inp = 20    # h_out\n",
    "f_out1 = 9   # number of possible actions\n",
    "f_out2 = 1   # value of state\n",
    "\n",
    "g_inp1 = 20   #h_out\n",
    "g_inp2 = 9   #f_out1\n",
    "g_out1 = 20   #h_out\n",
    "g_out2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = representation_h(h_inp,h_out)\n",
    "F = prediction_f(f_inp,f_out1,f_out2)\n",
    "G = dynamic_g(g_inp1,g_inp2,g_out1,g_out2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_proc(observation):\n",
    "    rgb_weights = [1,1,1]\n",
    "    gray_img = np.dot(observation,rgb_weights)\n",
    "    temp_obs = np.reshape(gray_img,(1,1,210,160))\n",
    "    temp_obs1 = perception(torch.FloatTensor(temp_obs))\n",
    "    temp_obs2 = perception(torch.FloatTensor(temp_obs1))  \n",
    "    return temp_obs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_param = list(F.parameters())\n",
    "g_param = list(G.parameters())\n",
    "h_param = list(H.parameters())\n",
    "\n",
    "criterion_p = nn.MSELoss()\n",
    "optimizer_p = optim.Adam(f_param+g_param+h_param, lr=0.01)\n",
    "\n",
    "criterion_v = nn.MSELoss()\n",
    "optimizer_v = optim.Adam(f_param+g_param+h_param, lr=0.01)\n",
    "\n",
    "criterion_r = nn.MSELoss()\n",
    "optimizer_r = optim.Adam(g_param+h_param, lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Buffer = []\n",
    "loss_list = []\n",
    "game_name = 'Asteroids-v0'\n",
    "game_name = 'MsPacman-v0'\n",
    "gamma = .99\n",
    "epochs = 100\n",
    "steps = 1\n",
    "depth = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0\n",
      "tensor(0.9379, grad_fn=<AddBackward0>)\n",
      "EPOCH 1\n",
      "tensor(0.9690, grad_fn=<AddBackward0>)\n",
      "EPOCH 2\n",
      "tensor(0.9660, grad_fn=<AddBackward0>)\n",
      "EPOCH 3\n",
      "tensor(0.8982, grad_fn=<AddBackward0>)\n",
      "EPOCH 4\n",
      "tensor(0.9439, grad_fn=<AddBackward0>)\n",
      "EPOCH 5\n",
      "tensor(0.9577, grad_fn=<AddBackward0>)\n",
      "EPOCH 6\n",
      "tensor(0.9171, grad_fn=<AddBackward0>)\n",
      "EPOCH 7\n",
      "tensor(0.9261, grad_fn=<AddBackward0>)\n",
      "EPOCH 8\n",
      "tensor(0.9675, grad_fn=<AddBackward0>)\n",
      "EPOCH 9\n",
      "tensor(0.9210, grad_fn=<AddBackward0>)\n",
      "EPOCH 10\n",
      "tensor(0.9157, grad_fn=<AddBackward0>)\n",
      "EPOCH 11\n",
      "tensor(0.9316, grad_fn=<AddBackward0>)\n",
      "EPOCH 12\n",
      "tensor(0.8956, grad_fn=<AddBackward0>)\n",
      "EPOCH 13\n",
      "tensor(0.9182, grad_fn=<AddBackward0>)\n",
      "EPOCH 14\n",
      "tensor(0.8793, grad_fn=<AddBackward0>)\n",
      "EPOCH 15\n",
      "tensor(0.9147, grad_fn=<AddBackward0>)\n",
      "EPOCH 16\n",
      "tensor(0.9088, grad_fn=<AddBackward0>)\n",
      "EPOCH 17\n",
      "tensor(0.9283, grad_fn=<AddBackward0>)\n",
      "EPOCH 18\n",
      "tensor(0.8890, grad_fn=<AddBackward0>)\n",
      "EPOCH 19\n",
      "tensor(0.9176, grad_fn=<AddBackward0>)\n",
      "EPOCH 20\n",
      "tensor(0.9135, grad_fn=<AddBackward0>)\n",
      "EPOCH 21\n",
      "tensor(0.9450, grad_fn=<AddBackward0>)\n",
      "EPOCH 22\n",
      "tensor(0.9364, grad_fn=<AddBackward0>)\n",
      "EPOCH 23\n",
      "tensor(0.9182, grad_fn=<AddBackward0>)\n",
      "EPOCH 24\n",
      "tensor(0.9320, grad_fn=<AddBackward0>)\n",
      "EPOCH 25\n",
      "tensor(0.9019, grad_fn=<AddBackward0>)\n",
      "EPOCH 26\n",
      "tensor(0.9534, grad_fn=<AddBackward0>)\n",
      "EPOCH 27\n",
      "tensor(0.9186, grad_fn=<AddBackward0>)\n",
      "EPOCH 28\n",
      "tensor(0.9124, grad_fn=<AddBackward0>)\n",
      "EPOCH 29\n",
      "tensor(0.9364, grad_fn=<AddBackward0>)\n",
      "EPOCH 30\n",
      "tensor(0.9649, grad_fn=<AddBackward0>)\n",
      "EPOCH 31\n",
      "tensor(0.9315, grad_fn=<AddBackward0>)\n",
      "EPOCH 32\n",
      "tensor(0.9171, grad_fn=<AddBackward0>)\n",
      "EPOCH 33\n",
      "tensor(0.8960, grad_fn=<AddBackward0>)\n",
      "EPOCH 34\n",
      "tensor(0.9375, grad_fn=<AddBackward0>)\n",
      "EPOCH 35\n",
      "tensor(0.9261, grad_fn=<AddBackward0>)\n",
      "EPOCH 36\n",
      "tensor(0.9548, grad_fn=<AddBackward0>)\n",
      "EPOCH 37\n",
      "tensor(0.9591, grad_fn=<AddBackward0>)\n",
      "EPOCH 38\n",
      "tensor(0.9270, grad_fn=<AddBackward0>)\n",
      "EPOCH 39\n",
      "tensor(0.9585, grad_fn=<AddBackward0>)\n",
      "EPOCH 40\n",
      "tensor(0.9459, grad_fn=<AddBackward0>)\n",
      "EPOCH 41\n",
      "tensor(0.9524, grad_fn=<AddBackward0>)\n",
      "EPOCH 42\n",
      "tensor(0.9454, grad_fn=<AddBackward0>)\n",
      "EPOCH 43\n",
      "tensor(0.9364, grad_fn=<AddBackward0>)\n",
      "EPOCH 44\n",
      "tensor(0.9268, grad_fn=<AddBackward0>)\n",
      "EPOCH 45\n",
      "tensor(0.9037, grad_fn=<AddBackward0>)\n",
      "EPOCH 46\n",
      "tensor(0.9768, grad_fn=<AddBackward0>)\n",
      "EPOCH 47\n",
      "tensor(0.8987, grad_fn=<AddBackward0>)\n",
      "EPOCH 48\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"EPOCH {}\".format(epoch))\n",
    "    \n",
    "    #start game\n",
    "    env = gym.make(game_name)\n",
    "    observation = env.reset()\n",
    "    \n",
    "    #get players and state\n",
    "    ship = player()\n",
    " \n",
    "    #collect lists\n",
    "    policy_list = []\n",
    "    action_list = []\n",
    "\n",
    "    value_list = []\n",
    "    z_list = []\n",
    "\n",
    "    reward_list = []\n",
    "    imm_reward_list = []\n",
    "\n",
    "    done = 0\n",
    "    for i in range(100):\n",
    "        \n",
    "        #data preprocessing\n",
    "        H_board = data_pre_proc(observation)\n",
    "\n",
    "        #build tree\n",
    "        ship.build_tree(H,F,G,H_board,depth,f_out1)\n",
    "\n",
    "        #Find best move\n",
    "        action = get_trajectory(ship.Graph,0,action_list,value_list,policy_list,reward_list,f_out1)    \n",
    "        temp_depth = depth\n",
    "        for j in range(depth-1):\n",
    "            #take action\n",
    "            action = arr_2_action(action_list[j])\n",
    "            #env.render()\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            imm_reward_list.append(reward)\n",
    "            if(done):\n",
    "                temp_depth = j\n",
    "                break\n",
    "        \n",
    "        z_list = get_z(imm_reward_list,gamma)\n",
    "        \n",
    "        if(done):\n",
    "            break\n",
    "        \n",
    "        #list checking\n",
    "        #print(\"z: {} v: {} \".format(len(z_list),len(value_list)))\n",
    "        #print(\"im_r: {} r: {}\".format(len(imm_reward_list),len(reward_list)))\n",
    "        #print(\"policy: {} action: {}\".format(len(policy_list),len(action_list)))\n",
    "    \n",
    "        #print(torch.FloatTensor([z_list[i]]))\n",
    "        \n",
    "    \n",
    "    #train\n",
    "    Total_loss = 0\n",
    "    for i in range(temp_depth-1):\n",
    "        policy_loss = criterion_p(action_list[i],policy_list[i])\n",
    "        value_loss = criterion_v(torch.FloatTensor([z_list[i]]),value_list[i])\n",
    "        reward_loss = criterion_r(torch.FloatTensor([imm_reward_list[i]]),reward_list[i])\n",
    "        Total_loss+= policy_loss + value_loss + reward_loss\n",
    "    \n",
    "    optimizer_p.step()\n",
    "    optimizer_v.step()\n",
    "    optimizer_r.step()\n",
    "    \n",
    "    loss_list.append(Total_loss)   \n",
    "    env.close()\n",
    "    print(Total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(loss_list))],loss_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = 0\n",
    "while(done == False):\n",
    "\n",
    "    #data preprocessing\n",
    "    H_board = data_pre_proc(observation)\n",
    "\n",
    "    #build tree\n",
    "    ship.build_tree(H,F,G,H_board,depth,f_out1)\n",
    "\n",
    "    #Find best move\n",
    "    action = get_trajectory(ship.Graph,0,action_list,value_list,policy_list,reward_list,f_out1)   \n",
    "    \n",
    "    env.render()\n",
    "    time.sleep(.01)\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    if(done):\n",
    "        env.close()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
